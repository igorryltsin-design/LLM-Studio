# LLM Studio 1.2.12

Современная рабочая станция для локального и гибридного использования LLM. Приложение объединяет веб-интерфейс (React + Vite) и backend на FastAPI, позволяя:

- запускать чат с локальными моделями (Gemma и совместимые архитектуры);
- подключать внешние API (OpenAI-совместимые, Hugging Face Inference, внутренние сервера);
- собирать, очищать и размечать датасеты прямо в интерфейсе;
- дообучать модели с помощью LoRA/QLoRA/Full fine-tuning;
- отслеживать ресурсы GPU/CPU/RAM и состояние задач в реальном времени.

## Обзор функций

### Чат
- Сценарии «Базовая модель», «Адаптер (LoRA/QLoRA)», «Удалённый API» — можно сравнивать ответы параллельно.
- История диалогов и статусов запросов (успех, отмена, ошибка) с быстрым фильтром.
- Настройки генерации: `max_tokens`, `temperature`, `top_p`, ограничение по устройству `cpu/cuda/mps`.

### Управление датасетами
- Импорт PDF/DOCX/TXT/JSONL.
- Автогенерация Q&A на основе загруженного текста.
- Полуавтоматическая модерация: отклонение, правки и тегирование примеров.
- Экспорт в тренировочный датасет одной кнопкой.

### Fine-tuning
- Методы: `LoRA`, `QLoRA`, `Full`.
- Настройка гиперпараметров (rank, alpha, batch_size, epochs, lr, warmup).
- Мониторинг прогресса: Loss, Perplexity, скорость итераций, статус адаптера.
- Возможность продолжить обучение с предыдущего адаптера (resume).

### Мониторинг и служебные функции
- Панель ресурсов (CPU, RAM, GPU, диски, занятые порты).
- История задач и логов с экспортом в JSON/Markdown.
- Модуль «Agregator»: подключение корпоративного источника данных для автоматической выгрузки и обучения.

## Архитектура проекта

```
LLM Studio
├── src/                   # современный фронтенд (React, TypeScript, Tailwind)
│   ├── components/        # вкладки интерфейса, статусные панели, модальные окна
│   ├── contexts/          # глобальные сторы (chat, datasets, training, settings)
│   ├── hooks/             # кастомные хуки для API и состояния
│   ├── services/          # HTTP-клиенты для backend эндпоинтов
│   ├── types/             # типы и схемы DTO
│   └── utils/             # вспомогательные функции
├── dist/                  # статический билд фронтенда (переопределяется при сборке)
├── base_model_server.py   # FastAPI + Uvicorn (чат, fine-tune, статические файлы)
├── fine_tune_manager.py   # управление задачами обучения и сохранение адаптеров
├── Models/                # каталог с базовыми моделями и результатами дообучения
├── build_and_run.sh       # общая утилита для локального запуска (npm + uvicorn)
├── build_and_save_amd64.sh# сборка Docker-образа с CUDA (amd64)
├── docker-entrypoint.sh   # точка входа внутри контейнера
├── scripts/               # вспомогательные shell-скрипты (загрузка Linux wheels)
└── requirements.txt       # python-зависимости backend-а
```

Backend поднимает два типа эндпоинтов:
- `/v1/...` и `/system/...` — REST API для UI.
- `/` и статические маршруты — отдают production-бандл.

## Требования

| Компонент | Минимальная версия | Примечания |
|-----------|--------------------|-----------|
| Node.js   | 18.x (рекомендовано 20.x) | используется Vite и Tailwind |
| Python    | 3.10+ | проверено на 3.10/3.11 |
| CUDA      | 12.1+ для GPU-режима | Torch 2.4.0 + cu121 |
| Git, Bash | актуальные | для скриптов и управления зависимостями |
| Docker    | 24+ (опционально) | для переноса окружения |
| NVIDIA Driver | 535+ | при использовании GPU |

Дополнительно:
- Linux: `bitsandbytes` требует glibc >= 2.27.
- macOS (Apple Silicon): используется MPS, CUDA недоступна.
- Windows: рекомендуется WSL2 (Ubuntu 22.04) для CUDA сценариев.

## Подготовка окружения

### 1. Клонирование и установка зависимостей
```bash
git clone https://github.com/<your-account>/llm-studio.git
cd llm-studio
```

**Frontend**
```bash
npm install
```

**Backend (виртуальное окружение Python)**
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```
> Для GPU заранее установите `torch==2.4.0` из CUDA-индекса: `pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121`.

### 2. Подготовка каталога моделей
```
llm-studio/
└── Models/
    ├── base/            # базовые модели (Gemma, Llama, Mistral...)
    └── adapters/        # результаты fine-tuning
```
Добавьте путь к нужной модели в UI («Настройки» → «Каталог моделей»).

## Запуск приложения на разных платформах

### Linux (CUDA)
1. Убедитесь, что установлены NVIDIA Driver, CUDA Toolkit (только драйвер) и `nvidia-container-toolkit` (если планируете Docker).
2. Активируйте виртуальное окружение и установите зависимости (см. выше).
3. Запустите backend и frontend:
   ```bash
   ./build_and_run.sh --install   # установит npm и Python зависимости, соберёт фронт, поднимет uvicorn
   ```
4. Интерфейс будет доступен на `http://localhost:4173` (режим предпросмотра) или на `http://localhost:5173` при запуске с `./build_and_run.sh --dev`.
5. Сервер базовой модели слушает `http://127.0.0.1:8001`.

**Советы для Linux**
- Если нужно только собрать фронтенд: `npm run build`.
- Для запуска backend без фронта: `UVICORN_APP=base_model_server:app uvicorn base_model_server:app --reload`.
- Для установки CUDA-зависимостей офлайн: `./scripts/download_linux_wheels.sh` (создаёт `vendor/pip`).

### macOS (Intel и Apple Silicon)
1. Установите Node.js 20 и Python 3.11 (через Homebrew).
2. Выполните шаги установки зависимостей.
3. Запуск:
   ```bash
   ./build_and_run.sh --install --skip-base   # если нужно только фронтенд
   ./build_and_run.sh --install               # поднимет uvicorn (использует CPU или MPS)
   ```
4. Для использования MPS (Apple Silicon) установите `torch` с поддержкой `mps` из официальных колёс PyTorch (`pip install torch torchvision torchaudio`).
5. bitsandbytes на macOS недоступен. QLoRA/4bit режимы работают только на Linux.

### Windows
1. Рекомендуется WSL2 (Ubuntu 22.04):
   ```powershell
   wsl --install -d Ubuntu-22.04
   ```
2. Внутри WSL выполните Linux-инструкцию. CUDA для WSL требует драйверов NVIDIA 535+ и `wsl --update`.
3. Альтернатива — запуск через Docker (см. раздел ниже).
4. Для нативного PowerShell есть скрипт `build_and_run.ps1`, который выполняет аналогичные шаги (npm install, сборка, запуск uvicorn). Запускайте из Powershell с правами выполнения:
   ```powershell
   Set-ExecutionPolicy -Scope Process -ExecutionPolicy RemoteSigned
   .\build_and_run.ps1 -Install
   ```

### Docker (amd64 + CUDA)
Используйте готовый скрипт для сборки и переноса образа:
```bash
# (опционально) подготовить Linux wheels для офлайн-сборки
./scripts/download_linux_wheels.sh

# собрать образ и сохранить в tar
./build_and_save_amd64.sh

# перенести файл llm-studio-cuda-amd64.tar на целевую машину
scp llm-studio-cuda-amd64.tar user@host:/tmp
```
На целевой машине:
```bash
docker load -i llm-studio-cuda-amd64.tar

docker run --gpus all \
  -p 8001:8001 \
  -v /path/to/Models:/opt/llm-studio/Models \
  -v ~/.cache/huggingface:/opt/llm-studio/cache/huggingface \ # опционально
  --name llm-studio \
  llm-studio:cuda-amd64
```
Контейнер включает production-бандл UI и FastAPI-сервер. Интерфейс доступен по адресу `http://localhost:8001`; при необходимости можно прокинуть reverse-proxy (например, Nginx) либо запускать фронтенд локально (`npm run dev`) и подключаться к backend в контейнере.

**Переменные окружения контейнера**
- `UVICORN_HOST`, `UVICORN_PORT`, `UVICORN_WORKERS` — параметры uvicorn.
- `UVICORN_EXTRA_ARGS` — дополнительные аргументы (`--log-level debug`, `--reload` и т.д.).
- `MODELS_DIR` — переопределяет каталог с моделями внутри контейнера (по умолчанию `/opt/llm-studio/Models`).
- `HF_HOME`, `TRANSFORMERS_CACHE`, `TORCH_HOME` — директории для кеша моделей.

## Работа с моделями

1. Скачайте модель (например, Gemma 2B) через `huggingface-cli` или вручную.
2. Разместите файлы в `Models/<model-name>`.
3. В UI укажите путь (вкладка «Настройки» → «Каталог модели»).
4. Для адаптеров LoRA размещайте каталоги в `Models/adapters/<adapter-name>`.
5. Если доступ к приватным репо HF необходим, задайте переменную `HF_TOKEN` и запишите токен в `~/.huggingface/token` или экспортируйте в окружение перед запуском сервера.

## Fine-tuning: пошаговый сценарий

1. **Сбор датасета**
   - Загружайте файлы во вкладке «Датасеты» → «Импорт».
   - Используйте «Создать Q&A» для автогенерации примеров.
   - Проверьте и отредактируйте ответы, присвойте теги.
   - Добавьте примеры в «Обучающий набор» и экспортируйте.

2. **Настройка Fine-tune**
   - Перейдите в «Обучение».
   - Выберите метод (`LoRA`, `QLoRA`, `Full`).
   - Настройте гиперпараметры:
     - `lora_rank`, `lora_alpha`
     - `learning_rate`, `batch_size`, `epochs`
     - `warmup_steps`, `max_length`
   - Опционально укажите путь к предыдущему адаптеру (resume).

3. **Запуск и мониторинг**
   - Наблюдайте прогресс в реальном времени (графики Loss, Perplexity, ETA).
   - Логи выводятся в отдельной панели; ошибки подсвечиваются.
   - При успешном завершении адаптер сохраняется в `Models/finetune-<timestamp>`.

4. **Использование адаптера**
   - Во вкладке «Чат» выберите адаптер или подключите к базовой модели.
   - Переключение происходит динамически, без перезапуска сервера.

## Сценарии использования

### 1. Чат с локальной моделью
1. Скопируйте модель в `Models/base/gemma-2b/` (или другой каталог).
2. Запустите backend: `./build_and_run.sh --install` (Linux/macOS) или `start_base_model.sh --install`.
3. В UI откройте вкладку «Настройки» → «Каталог модели», введите путь `Models/base/gemma-2b`.
4. Перейдите во вкладку «Чат», выберите источник «Базовая модель» и начните диалог.
   ```text
   Пользователь: Придумай название для внутренней аналитической панели.
   Модель: «Insight Pulse» — короткое и отражает идею мониторинга.
   ```
5. Во время сессии можно переключать устройство выполнения (`cuda`, `cpu`, `mps`) прямо в настройках генерации.

### 2. Чат с удалённым API
1. Получите токен провайдера (например, OpenAI compatible endpoint).
2. В «Настройках» включите режим «Удалённый API», укажите URL (`https://api.example.com/v1/chat/completions`) и токен в формате `Bearer ...`.
3. Вкладка «Чат» → источник «Удалённый API».
4. Отправьте запрос и получите ответ из внешнего сервиса, сохранив историю в приложении.

### 3. Сравнение локальной модели и адаптера
1. Запустите локальный сервер (см. сценарий 1).
2. В UI загрузите или создайте адаптер (см. сценарий 5).
3. Во вкладке «Чат» включите режим «Сравнение источников».
4. Задайте один вопрос — приложение отправит его базе и адаптеру, а результат выведет рядом для оценки качества.

### 4. Подготовка датасета из документа
1. Перейдите во вкладку «Датасеты» → «Импорт» и загрузите PDF или DOCX.
2. Нажмите «Создать Q&A» — система предложит автоматически сгенерированные пары вопрос/ответ.
3. Просмотрите каждую пару: отклоняйте некорректные, редактируйте формулировки, добавляйте теги (`billing`, `support`).
4. Отмеченные примеры отправьте в «Обучающий набор» и экспортируйте (кнопка «Сформировать датасет»).
5. Экспортированный набор появится во вкладке «Обучение» и будет доступен в сценариях fine-tuning.

### 5. Fine-tuning LoRA на основе свежих данных
1. Сформируйте набор (сценарий 4) или подгрузите CSV/JSONL.
2. Во вкладке «Обучение» выберите метод `LoRA`, укажите базовую модель, гиперпараметры и каталог сохранения (`Models/adapters/gemma-lora-support`).
3. Нажмите «Запустить обучение»; прогресс отображается в реальном времени.
4. После завершения адаптер появится в списке «Доступные адаптеры».
5. В чате выберите адаптер, чтобы проверить улучшения на контрольных запросах.

### 6. Автообновление через Agregator
1. Настройте доступ к корпоративному сервису: вкладка «Настройки» → «Agregator» → введите `base_url`, токен и нужный `collection`.
2. Во вкладке «Обучение» создайте задачу с типом «Agregator: авто сбор и обучение».
3. Приложение выгрузит свежие кейсы, объединит их со старым датасетом (если выбран `include_previous_dataset`), запустит fine-tuning и сохранит результат.
4. Статус выгрузки и обучения можно отслеживать в истории и логах.

### 7. Мониторинг и эксплуатация
1. Панель «Ресурсы» в боковом меню отображает загрузку CPU, RAM, GPU, температуру и активные задачи.
2. Для Docker-развёртывания подключите `docker stats` или `nvidia-smi` внутри контейнера:
   ```bash
   docker exec -it llm-studio nvidia-smi
   ```
3. Логи backend доступны в `/var/log/uvicorn.log` (если настроено перенаправление) или в стандартном выводе контейнера/скрипта.
4. История чата, fine-tuning и датасетов экспортируется в JSON/Markdown для отчётности.

## Вкладки интерфейса (UI)

- **Чат** — основное общение с моделью, переключение между режимами (база, адаптер, API).
- **Обучение** — запуск задач fine-tuning, просмотр истории и метрик.
- **Тестирование** — A/B сравнение ответов, чек-листы QA.
- **Датасеты** — менеджер файлов, Q&A генератор, ручная разметка, экспорт.
- **Настройки** — пути к моделям, параметры генерации, конфигурация API и Agregator.

## Ключевые скрипты

| Скрипт | Назначение |
|--------|------------|
| `build_and_run.sh` | Установка зависимостей, сборка фронтенда, запуск uvicorn (Linux/macOS). |
| `build_and_run.ps1` | Аналогичный сценарий для Windows PowerShell. |
| `start_base_model.sh` | Быстрый запуск только backend (устанавливает torch + зависимости). |
| `build_and_save_amd64.sh` | Сборка CUDA Docker-образа и сохранение в tar. |
| `scripts/download_linux_wheels.sh` | Предварительная загрузка Linux wheel-файлов (torch, bitsandbytes) для офлайн-сборки. |

## Управление зависимостями

### Frontend
- Обновить пакеты: `npm update`
- Проверка TypeScript: `npm run typecheck`
- ESLint: `npm run lint`

### Backend
- Закреплённый список в `requirements.txt`.
- Для обновления конкретного пакета: `pip install <pkg>==<версия>` и зафиксировать в файле.
- Рекомендуется использовать `pip-tools` для синхронизации (опционально).

## Отладка и тестирование

- Lint фронтенда перед коммитом: `npm run lint`.
- Тестирование backend вручную через `curl` или `httpie` (Swagger доступен по `/docs`).
- Проверка загрузки GPU: `watch nvidia-smi` (Linux, Docker `--gpus all`).
- Для проблем с портами используйте `lsof -i :8001`.

## Частые вопросы

**Можно ли запускать без GPU?**
Да, приложение работает на CPU. Генерация будет медленнее, а QLoRA не поддерживается без CUDA.

**Поддерживаются ли другие модели кроме Gemma?**
Да. Любая модель формата Hugging Face (Llama, Mistral, Falcon и т.д.), если её можно загрузить через `AutoModelForCausalLM` и `AutoTokenizer`.

**Как сменить порт фронтенда?**
Редактируйте `vite.config.ts` (`server.port`) или запускайте `npm run dev -- --port <номер>`.

**Как очистить кэш?**
Удалите каталоги `cache/huggingface`, `cache/torch`, `~/.cache/huggingface` (если подключали). Для Docker — пересоберите образ.

**Как обновить Torch?**
Обновите переменные `TORCH_VERSION` и `TORCH_INDEX_URL` перед запуском `start_base_model.sh --install` или внутри Dockerfile.

## Поддержка и контакты

Интерфейс разработан Ryltsin I.A. Предложения и баг-репорты присылайте через Issues или Pull Requests в репозитории.
